{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, derivative = False):\n",
    "    res = x\n",
    "    if derivative == True:\n",
    "        return (res > 0)*1\n",
    "    else:\n",
    "        return res * (res > 0)\n",
    "\n",
    "def sigmoid(x, derivative = False):\n",
    "    res = 1/(1 + np.exp(-x))\n",
    "    if derivative == True:\n",
    "        return res * (1 - res)\n",
    "    else:\n",
    "        return res\n",
    "    \n",
    "def first_moment_update(previous_moment, grad, beta, timestep):\n",
    "    biased = beta * previous_moment + (1 - beta) * grad\n",
    "    unbiased = biased / (1 - np.power(beta, timestep))\n",
    "    return unbiased\n",
    "\n",
    "def second_moment_update(previous_moment, grad, beta, timestep):\n",
    "    biased = beta * previous_moment + (1 - beta) * np.square(grad)\n",
    "    unbiased = biased / (1 - np.power(beta,timestep))\n",
    "    return unbiased\n",
    "\n",
    "def params_to_vector(dictionary):\n",
    "    counter = 0\n",
    "    for i in dictionary:\n",
    "        if counter == 0:\n",
    "            new_vec = dictionary[i].reshape(-1)\n",
    "        else:\n",
    "            new_vec2 = dictionary[i].reshape(-1)\n",
    "            new_vec = np.concatenate([new_vec, new_vec2])\n",
    "        counter += 1\n",
    "    return new_vec\n",
    "\n",
    "def forward_propagation(inputs, parameters):\n",
    "    W_1, b_1, W_mu, b_mu, W_sigma, b_sigma, W_2, b_2 = parameters.values()\n",
    "    m = inputs.shape[1]\n",
    "    \n",
    "    z_1 = W_1.dot(inputs) + b_1\n",
    "    a_1 = relu(z_1, derivative = False)\n",
    "    \n",
    "    z_mu = W_mu.dot(a_1) + b_mu\n",
    "    z_sigma = W_sigma.dot(a_1) + b_sigma\n",
    "\n",
    "    eps = np.random.randn(n_z, len(inputs.T))\n",
    "    sampled_vector = z_mu + np.multiply(np.exp(z_sigma * 0.5), eps) \n",
    "\n",
    "    z_2 = W_2.dot(sampled_vector) + b_2\n",
    "    \n",
    "    cost = np.mean(np.square(inputs - z_2))\n",
    "    \n",
    "    cache = (W_1, b_1, W_mu, b_mu, W_sigma, b_sigma, W_2, b_2, z_1, a_1, z_mu, z_sigma, sampled_vector, z_2)\n",
    "    \n",
    "    return cost, cache\n",
    "\n",
    "def backward_propagation(inputs, cache):\n",
    "    W_1, b_1, W_mu, b_mu, W_sigma, b_sigma, W_2, b_2, z_1, a_1, z_mu, z_sigma, sampled_vector, z_2 = cache\n",
    "    \n",
    "    m = inputs.shape[1]\n",
    "    grad_z_2 = 2* (z_2 - inputs) # (n_x, m)\n",
    "    grad_W_2 = (1/m) * grad_z_2.dot(sampled_vector.T) # (n_z, m)\n",
    "    grad_b_2 = (1/m) * np.sum(grad_z_2, axis=1, keepdims = True)\n",
    "    grad_s =  W_2.T.dot(grad_z_2) # (n_z, m), gradient of sampled vector\n",
    "    grad_z_mu = grad_s # (n_z, m)\n",
    "    grad_z_sigma = np.multiply(grad_s, 0.5 * np.exp(z_sigma * 0.5) * eps)\n",
    "    grad_W_mu = (1/m) * grad_z_mu.dot(a_1.T)\n",
    "    grad_b_mu = (1/m) * np.mean(grad_z_mu, axis = 1,keepdims=True)\n",
    "    grad_W_sigma = grad_z_sigma.dot(a_1.T)\n",
    "    grad_b_sigma = np.mean(grad_z_sigma, axis = 1, keepdims=True)\n",
    "    grad_a_1 = W_mu.T.dot(grad_z_mu)\n",
    "    grad_z_1 = np.multiply(grad_a_1, relu(z_1, derivative=True))\n",
    "    grad_b_1 = (1/m) * np.mean(grad_z_1, axis = 1, keepdims = True)\n",
    "    grad_W_1 = (1/m) * grad_z_1.dot(batch.T)\n",
    "    \n",
    "    grads['W_1'] = grad_W_1\n",
    "    grads['b_1'] = grad_b_1\n",
    "    grads['W_mu'] = grad_W_mu\n",
    "    grads['b_mu'] = grad_b_mu\n",
    "    grads['W_sigma'] = grad_W_sigma\n",
    "    grads['b_sigma'] = grad_b_sigma\n",
    "    grads['W_2'] = grad_W_2\n",
    "    grads['b_2'] = grad_b_2\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 1 # Number of features\n",
    "m = 128 # Number of data points\n",
    "l_1 = 20 # Number of neurons in 1st layer\n",
    "n_z = 10 # Number of latent variables\n",
    "alpha = 1e-1 # learning_rate\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "kl_weight = 5e-3\n",
    "\n",
    "# Adam parameters\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "little_eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing that everything works before converting to a class\n",
    "np.random.seed(0)\n",
    "train_inputs = np.random.randn(m, n_x)\n",
    "train_inputs = train_inputs.T\n",
    "\n",
    "learned_params = {}\n",
    "# learned_params['W_1'] = np.random.randn(l_1, n_x) * np.sqrt(2/n_x) # Kaiming initialization\n",
    "learned_params['W_1'] = np.random.uniform(low = - np.sqrt(6 / (l_1 + n_x)),\n",
    "                                          high = np.sqrt(6 / (l_1 + n_x)),\n",
    "                                          size = (l_1, n_x)) # Xavier\n",
    "learned_params['b_1'] = np.zeros(shape = (l_1,1))\n",
    "# learned_params['W_mu'] = np.random.randn(n_z, l_1) * np.sqrt(2/l_1) # Kaiming initialization\n",
    "learned_params['W_mu'] = np.random.uniform(low = - np.sqrt(6 / (l_1 + n_z)),\n",
    "                                           high = np.sqrt(6 / (l_1 + n_z)),\n",
    "                                           size = (n_z, l_1)) # Xavier\n",
    "learned_params['b_mu'] = np.zeros(shape = (n_z, 1))\n",
    "# learned_params['W_sigma'] = np.random.randn(n_z, l_1) * np.sqrt(2/l_1) # Kaiming initialization\n",
    "learned_params['W_sigma'] = np.random.uniform(low = -np.sqrt(6 / (l_1 + n_z)),\n",
    "                                              high = np.sqrt(6 / (l_1 + n_z)),\n",
    "                                              size = (n_z, l_1)) # Xavier\n",
    "learned_params['b_sigma'] = np.zeros(shape = (n_z,1))\n",
    "# learned_params['W_2'] = np.random.randn(n_x, n_z) * np.sqrt(2/n_z) # Kaiming initialization\n",
    "learned_params['W_2'] = np.random.uniform(low = -np.sqrt(6 / (n_x + n_z)),\n",
    "                                          high = np.sqrt(6 / (n_x + n_z)),\n",
    "                                          size = (n_x, n_z)) # Xavier\n",
    "learned_params['b_2'] = np.zeros(shape = (n_x, 1))\n",
    "\n",
    "# Still seems too big, so I'm going to dock everything by 1e-2\n",
    "# learned_params = {i:j*1e-1 for i, j in learned_params.items()}\n",
    "\n",
    "learnable_params = learned_params.keys()\n",
    "\n",
    "first_moments = {i:0 for i in learnable_params}\n",
    "second_moments = {i:0 for i in learnable_params}\n",
    "grads = {i:0 for i in learnable_params}\n",
    "\n",
    "# train_truth = (np.random.sample(size=(n_x,1)) >= 0.5)*1\n",
    "\n",
    "losses_mse = []\n",
    "losses_kl = []\n",
    "losses = []\n",
    "\n",
    "# Implement mini-batch gradient descent.\n",
    "batches_per_epoch = (m // batch_size) + 1 - (1 * (isinstance(m / batch_size, int)))\n",
    "timestep = 1 # Keeps track of how many updates have been done, used by Adam optimizer.\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(1):\n",
    "        if j == (batches_per_epoch - 1): # Last batch\n",
    "            batch = train_inputs[:, j* batch_size:]\n",
    "        else:\n",
    "            batch = train_inputs[:, j * batch_size: (j+1) * batch_size] # All features, current batch\n",
    "        \n",
    "        W_1, b_1, W_mu, b_mu, W_sigma, b_sigma, W_2, b_2 = learned_params.values() # Get the current set of values.\n",
    "            \n",
    "        # Forward pass\n",
    "        z_1 = W_1.dot(batch) + b_1\n",
    "        a_1 = relu(z_1)\n",
    "\n",
    "        z_mu = W_mu.dot(a_1) + b_mu\n",
    "        # There is no activation for mu layer\n",
    "        z_sigma = W_sigma.dot(a_1) + b_sigma\n",
    "        \n",
    "        eps = np.random.randn(n_z, len(batch.T)) # Get the number of rows\n",
    "        sampled_vector = z_mu + np.multiply(np.exp(z_sigma * 0.5), eps) # Elem-wise multiplication for eps and var.\n",
    "        # Also treat z_sigma as a log-var instead, bypass the problem with negatives.\n",
    "\n",
    "        z_2 = W_2.dot(sampled_vector) + b_2\n",
    "        # Should there be a final activation? \n",
    "        # I don't think so - if we're going for MSE loss we're going to compare every single feature vector with the \n",
    "        # reconstructed one. So the range of the activation function has to be the whole real line, but most common ones\n",
    "        # are not that way, like ReLU, tanh, sigmoid.\n",
    "\n",
    "        # Loss - go with simple L2 loss\n",
    "        loss_mse = np.mean(np.square(batch - z_2)) # One number\n",
    "        loss_kl = 0.5*(np.sum(np.exp(z_sigma) + np.square(z_mu) - 1 - z_sigma, axis=0)) # m numbers\n",
    "        loss = loss_mse + kl_weight * loss_kl # (m,) shape\n",
    "\n",
    "        losses_mse.append(loss_mse)\n",
    "        losses_kl.append(np.mean(loss_kl))\n",
    "        losses.append(loss_mse + np.mean(loss_kl))\n",
    "        # Warning - KL loss, taking mean is obscuring how some are close to N(0,1) and some are not.\n",
    "\n",
    "#         if i % 5 == 0:\n",
    "#             print('The loss from epoch {} batch {} is: '.format(i,j) + str(loss))\n",
    "\n",
    "        # Backward pass\n",
    "        # Need one for W_1, b_1, W_mu, b_mu, W_sigma, b_sigma, W_2, b_2\n",
    "        # This is for MSE loss      \n",
    "        grad_z_2 = 2* (z_2 - batch) # (n_x, m)\n",
    "        grad_W_2 = grad_z_2.dot(sampled_vector.T) # (n_z, m)\n",
    "        grad_b_2 = np.mean(grad_z_2, axis=1, keepdims = True)\n",
    "        grad_s =  W_2.T.dot(grad_z_2) # (n_z, m), gradient of sampled vector\n",
    "        grad_z_mu = grad_s # (n_z, m)\n",
    "        grad_z_sigma = np.multiply(grad_s, 0.5 * np.exp(z_sigma * 0.5) * eps)\n",
    "        grad_W_mu = grad_z_mu.dot(a_1.T)\n",
    "        grad_b_mu = np.mean(grad_z_mu, axis = 1,keepdims=True)\n",
    "        grad_W_sigma = grad_z_sigma.dot(a_1.T)\n",
    "        grad_b_sigma = np.mean(grad_z_sigma, axis = 1, keepdims=True)\n",
    "        grad_a_1 = W_mu.T.dot(grad_z_mu)\n",
    "        grad_z_1 = np.multiply(grad_a_1, relu(z_1, derivative=True))\n",
    "        grad_b_1 = np.mean(grad_z_1, axis = 1, keepdims = True)\n",
    "        grad_W_1 = grad_z_1.dot(batch.T)\n",
    "\n",
    "#         # This is for KL loss\n",
    "#         grad_z_sigma +=  np.exp(z_sigma) - 1\n",
    "#         grad_z_mu += 2*z_mu # Seriously, wth. There is likely something from KL that can give intuition on this.\n",
    "#         grad_W_sigma += grad_z_sigma.dot(a_1.T)\n",
    "#         grad_b_sigma += np.squeeze(np.sum(grad_z_sigma, keepdims = True))\n",
    "#         grad_W_mu += grad_z_mu.dot(a_1.T)\n",
    "#         grad_b_mu += np.squeeze(np.sum(grad_z_mu, keepdims = True))\n",
    "\n",
    "        # Store all the gradients (Anyway to write this code cleaner?)\n",
    "        grads['W_1'] = grad_W_1\n",
    "        grads['b_1'] = grad_b_1\n",
    "        grads['W_mu'] = grad_W_mu\n",
    "        grads['b_mu'] = grad_b_mu\n",
    "        grads['W_sigma'] = grad_W_sigma\n",
    "        grads['b_sigma'] = grad_b_sigma\n",
    "        grads['W_2'] = grad_W_2\n",
    "        grads['b_2'] = grad_b_2\n",
    "        \n",
    "#         for param in learnable_params:\n",
    "#             learned_params[param] -= alpha * grads[param]\n",
    "        alpha *= np.sqrt(1-np.power(beta_2,timestep)) / (1 - np.power(beta_1,timestep))\n",
    "        for param in learnable_params:\n",
    "            \n",
    "            first_moments[param] = first_moment_update(previous_moment = first_moments[param], \n",
    "                                                       grad = grads[param],\n",
    "                                                       beta = beta_1,\n",
    "                                                       timestep = timestep)\n",
    "            second_moments[param] = second_moment_update(previous_moment = second_moments[param],\n",
    "                                                         grad = grads[param],\n",
    "                                                         beta = beta_2,\n",
    "                                                         timestep = timestep)\n",
    "            \n",
    "            learned_params[param] -= alpha * np.divide(first_moments[param],(np.sqrt(second_moments[param]) + little_eps))\n",
    "    timestep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.701392988438776"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 1e-7\n",
    "parameters_values = params_to_vector(learned_params)\n",
    "grad = params_to_vector(grads)\n",
    "num_parameters = len(parameters_values)\n",
    "J_plus = np.zeros((num_parameters,1))\n",
    "J_minus = np.zeros((num_parameters,1))\n",
    "gradapprox = np.zeros((num_parameters,1))\n",
    "\n",
    "for i in range(num_parameters):\n",
    "    thetaplus = np.copy(parameters_values)\n",
    "    thetaplus[i] = thetaplus[i] + epsilon\n",
    "    J_plus[i], _ = forward_propagation(train_inputs, learned_params)\n",
    "    \n",
    "    thetaminus = np.copy(parameters_values)\n",
    "    thetaminus[i] = thetaminus[i] - epsilon\n",
    "    J_minus[i], _ = forward_propagation(train_inputs, learned_params)\n",
    "    \n",
    "    gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 *(epsilon))\n",
    "    \n",
    "# Compare gradapprox to backward propagation gradients\n",
    "numerator = np.linalg.norm(grad - gradapprox)\n",
    "denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching TF and base\n",
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(128, 1)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (128, 20)            40          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (128, 10)            210         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (128, 10)            210         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (128, 10)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (128, 1)             11          lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 471\n",
      "Trainable params: 471\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution() # Must be called at program startup?\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "# Somehow if you use keras.layers it'll throw an error. I guess if you are using eager execution, use tensorflow\n",
    "# everywhere. Side: Stuff this.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_logsigma = args\n",
    "    tf.set_random_seed(0)\n",
    "    epsilon = tf.random_normal(shape = tf.shape(z_mean))\n",
    "    sampled_vector = tf.add(z_mean, tf.multiply(tf.exp(.5 * z_logsigma), epsilon))\n",
    "    return sampled_vector\n",
    "\n",
    "def total_vae_loss (x, x_pred, mu, logsigma, kl_weight =5e-3):\n",
    "    kl_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1 - logsigma, axis = 1)\n",
    "    reconstruction_loss = tf.reduce_mean((x - x_pred)**2)\n",
    "    total_vae_loss = kl_weight * kl_loss + reconstruction_loss\n",
    "    \n",
    "    losses = {'kl_loss': kl_loss,\n",
    "              'rc_loss': reconstruction_loss,\n",
    "              'total_vae_loss': total_vae_loss}\n",
    "    return losses\n",
    "\n",
    "set_random_seed(0)\n",
    "inputs = Input(shape = n_x, batch_size = batch_size) # Have to put batch size first, eh.\n",
    "\n",
    "a_1 = Dense(units = l_1, activation = 'relu')(inputs)\n",
    "z_mean = Dense(units = n_z)(a_1)\n",
    "z_logsigma = Dense(units = n_z)(a_1)\n",
    "\n",
    "# Implement the sampling layer\n",
    "sampled_vector = Lambda(sampling)([z_mean, z_logsigma])\n",
    "\n",
    "z_2 = Dense(units = n_x)(sampled_vector)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = [z_2, z_mean, z_logsigma])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# model.compile(optimizer = tf.train.AdamOptimizer(learning_rate = alpha),\n",
    "#               loss = total_vae_loss) # No idea how to make loss work yet\n",
    "\n",
    "# np.random.seed(0)\n",
    "# train_inputs = np.random.randn(m, n_x)\n",
    "# train_inputs = tf.convert_to_tensor(train_inputs, dtype = np.float32)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate = alpha)\n",
    "# total_loss_values = []\n",
    "# rc_loss_values = []\n",
    "# kl_loss_values = []\n",
    "\n",
    "# batches_per_epoch = (m // batch_size) + 1 - (1 * (isinstance(m / batch_size, int)))\n",
    "\n",
    "# for i in range(1):\n",
    "#     for j in range(1):\n",
    "#         if j == (batches_per_epoch - 1): # Last batch\n",
    "#             batch = train_inputs[j* batch_size:, :]\n",
    "#         else:\n",
    "#             batch = train_inputs[j * batch_size: (j+1) * batch_size, :] # All features, current batch\n",
    "            \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             z_2, z_mean, z_logsigma = model(batch)\n",
    "#             losses = total_vae_loss(batch, z_2, z_mean, z_logsigma)\n",
    "#             grads = tape.gradient(losses['total_vae_loss'], model.weights) # model.weights means variables here.\n",
    "#             optimizer.apply_gradients(zip(grads, model.weights), global_step = tf.train.get_or_create_global_step())\n",
    "\n",
    "#             # Track the losses\n",
    "#             total_loss_values.append(losses['total_vae_loss'].numpy().mean()) # loss_value is a tensor with loss for every single data point.\n",
    "#             rc_loss_values.append(losses['rc_loss'].numpy().mean())\n",
    "#             kl_loss_values.append(losses['kl_loss'].numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "train_inputs = np.random.randn(m, n_x)\n",
    "\n",
    "# To be the same as tensorflow we have no choice but to initialize using its model weights, and steal it.\n",
    "learned_params = {}\n",
    "learned_params['W_1'] = model.get_weights()[0]\n",
    "learned_params['b_1'] = model.get_weights()[1]\n",
    "learned_params['W_mu'] = model.get_weights()[2]\n",
    "learned_params['b_mu'] = model.get_weights()[3]\n",
    "learned_params['W_sigma'] = model.get_weights()[4]\n",
    "learned_params['b_sigma'] = model.get_weights()[5]\n",
    "learned_params['W_2'] = model.get_weights()[6]\n",
    "learned_params['b_2'] = model.get_weights()[7]\n",
    "\n",
    "# for i in range(len(learned_params)):\n",
    "#     assert(learned_params[list(learned_params.keys())[i]].shape == model.get_weights()[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step - put both through forward propagation.\n",
    "\n",
    "Note that in Keras, the forward propagation is done in an X.W manner - see this:\n",
    "https://github.com/keras-team/keras/blob/master/keras/layers/core.py. In particular, we have the call method to Dense layer.\n",
    "\n",
    "Thus, we change our feedforward code to transpose the learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.65901931, 1.65351872, 1.65497699, 1.662714  , 1.65973859,\n",
       "       1.65525709, 1.6548752 , 1.65327835, 1.65325238, 1.65353405,\n",
       "       1.6532671 , 1.65713101, 1.65428151, 1.6532564 , 1.65358553,\n",
       "       1.65343048, 1.6573518 , 1.653319  , 1.65340641, 1.65477762,\n",
       "       1.66720818, 1.654004  , 1.65458949, 1.65439815, 1.66296949,\n",
       "       1.65772897, 1.65323356, 1.65330406, 1.65757258, 1.65721393,\n",
       "       1.65327296, 1.65348774, 1.6549023 , 1.6616034 , 1.65348636,\n",
       "       1.65327375, 1.65600629, 1.65587999, 1.6535478 , 1.6534235 ,\n",
       "       1.65556418, 1.65751819, 1.65943154, 1.66034969, 1.65378048,\n",
       "       1.65363662, 1.65656496, 1.65432782, 1.6587751 , 1.65332572,\n",
       "       1.65493141, 1.65349984, 1.65378298, 1.65619086, 1.65323148,\n",
       "       1.65356099, 1.65323774, 1.65339463, 1.65408306, 1.65350871,\n",
       "       1.65418883, 1.65350382, 1.65463259, 1.65957868, 1.65328641,\n",
       "       1.65357199, 1.65888823, 1.65361662, 1.65497674, 1.65323464,\n",
       "       1.65419447, 1.6532597 , 1.65560615, 1.6564697 , 1.65352189,\n",
       "       1.6542244 , 1.65483882, 1.65394026, 1.65343553, 1.65323546,\n",
       "       1.65611353, 1.65470747, 1.65362147, 1.65825206, 1.65731908,\n",
       "       1.65994334, 1.65577556, 1.65329841, 1.65566427, 1.65526112,\n",
       "       1.65357437, 1.65597048, 1.65330784, 1.65496943, 1.65345879,\n",
       "       1.65413542, 1.65323   , 1.65916719, 1.65325875, 1.65352138,\n",
       "       1.65985086, 1.65709146, 1.65666008, 1.65494346, 1.65615323,\n",
       "       1.660296  , 1.65359245, 1.65441488, 1.66014202, 1.65727585,\n",
       "       1.6597386 , 1.6547248 , 1.6548036 , 1.66004706, 1.65338203,\n",
       "       1.65440004, 1.65486529, 1.65328072, 1.65391268, 1.65477914,\n",
       "       1.65348537, 1.65579657, 1.65339004, 1.65646451, 1.65425297,\n",
       "       1.65327725, 1.65363121, 1.65960819])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_propagation_K(inputs, parameters, kl_weight = 5e-3):\n",
    "    W_1, b_1, W_mu, b_mu, W_sigma, b_sigma, W_2, b_2 = parameters.values()\n",
    "    \n",
    "    z_1 = np.dot(inputs, W_1) + b_1\n",
    "    a_1 = relu(z_1)\n",
    "    z_mu = np.dot(a_1, W_mu) + b_mu\n",
    "    z_sigma = np.dot(a_1, W_sigma) + b_sigma\n",
    "    \n",
    "    eps = tf.random_normal(shape = tf.shape(z_mu)).numpy()\n",
    "    sampled_vector = z_mu + np.multiply(np.exp(z_sigma * 0.5), eps)\n",
    "    \n",
    "    z_2 = np.dot(sampled_vector, W_2) + b_2\n",
    "    \n",
    "    cost = np.mean(np.square(inputs - z_2))\n",
    "    loss_kl = 0.5*(np.sum(np.exp(z_sigma) + np.square(z_mu) - 1 - z_sigma, axis=1))\n",
    "    total_vae_loss = cost + kl_weight * loss_kl\n",
    "    cache = {\n",
    "        'W_1': W_1,\n",
    "        'b_1': b_1,\n",
    "        'z_1': z_1,\n",
    "        'a_1': a_1,\n",
    "        'W_mu': W_mu,\n",
    "        'b_mu': b_mu,\n",
    "        'z_mu': z_mu,\n",
    "        'W_sigma': W_sigma,\n",
    "        'b_sigma': b_sigma,\n",
    "        'z_sigma': z_sigma,\n",
    "        'sampled_vector': sampled_vector,\n",
    "        'W_2': W_2,\n",
    "        'b_2': b_2,\n",
    "        'z_2': z_2,\n",
    "        'eps': eps,\n",
    "        'inputs':inputs,\n",
    "    }\n",
    "    return total_vae_loss, cache\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "forward_propagation_K(train_inputs, learned_params)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=318, shape=(128,), dtype=float32, numpy=\n",
       "array([1.6590192, 1.6535187, 1.654977 , 1.6627139, 1.6597385, 1.655257 ,\n",
       "       1.6548752, 1.6532782, 1.6532522, 1.6535339, 1.653267 , 1.657131 ,\n",
       "       1.6542814, 1.6532563, 1.6535854, 1.6534303, 1.6573517, 1.6533189,\n",
       "       1.6534063, 1.6547775, 1.6672081, 1.6540039, 1.6545894, 1.6543981,\n",
       "       1.6629694, 1.6577289, 1.6532335, 1.653304 , 1.6575725, 1.6572138,\n",
       "       1.6532729, 1.6534877, 1.6549022, 1.6616033, 1.6534863, 1.6532737,\n",
       "       1.6560062, 1.6558799, 1.6535478, 1.6534234, 1.6555641, 1.6575181,\n",
       "       1.6594315, 1.6603496, 1.6537803, 1.6536366, 1.6565648, 1.6543278,\n",
       "       1.658775 , 1.6533257, 1.6549313, 1.6534997, 1.6537828, 1.6561908,\n",
       "       1.6532314, 1.6535609, 1.6532377, 1.6533946, 1.654083 , 1.6535087,\n",
       "       1.6541888, 1.6535038, 1.6546324, 1.6595786, 1.6532863, 1.6535718,\n",
       "       1.6588881, 1.6536165, 1.6549766, 1.6532346, 1.6541944, 1.6532596,\n",
       "       1.655606 , 1.6564696, 1.6535218, 1.6542243, 1.6548387, 1.6539402,\n",
       "       1.6534355, 1.6532354, 1.6561134, 1.6547074, 1.6536214, 1.658252 ,\n",
       "       1.657319 , 1.6599432, 1.6557754, 1.6532984, 1.6556642, 1.655261 ,\n",
       "       1.6535742, 1.6559703, 1.6533078, 1.6549693, 1.6534587, 1.6541353,\n",
       "       1.65323  , 1.659167 , 1.6532587, 1.6535213, 1.6598507, 1.6570914,\n",
       "       1.65666  , 1.6549433, 1.6561531, 1.660296 , 1.6535923, 1.6544148,\n",
       "       1.660142 , 1.6572758, 1.6597385, 1.6547247, 1.6548035, 1.6600469,\n",
       "       1.653382 , 1.6544   , 1.6548653, 1.6532806, 1.6539125, 1.6547791,\n",
       "       1.6534853, 1.6557965, 1.6533899, 1.6564645, 1.6542529, 1.6532772,\n",
       "       1.6536311, 1.6596081], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def total_vae_loss (x, x_pred, mu, logsigma, kl_weight =5e-3):\n",
    "    kl_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1 - logsigma, axis = 1)\n",
    "    reconstruction_loss = tf.reduce_mean((x - x_pred)**2)\n",
    "    total_vae_loss = kl_weight * kl_loss + reconstruction_loss\n",
    "    \n",
    "    losses = {'kl_loss': kl_loss,\n",
    "              'rc_loss': reconstruction_loss,\n",
    "              'total_vae_loss': total_vae_loss}\n",
    "    return losses\n",
    "tf.set_random_seed(0)\n",
    "z_2, z_mu, z_sigma = model(train_inputs)\n",
    "losses = total_vae_loss(train_inputs, z_2, z_mu, z_sigma)\n",
    "losses['total_vae_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = alpha)\n",
    "tf.set_random_seed(0)\n",
    "model.load_weights('Initialized_model.h5')\n",
    "for i in range(1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z_2, z_mean, z_logsigma = model(train_inputs)\n",
    "        losses = total_vae_loss(train_inputs, z_2, z_mean, z_logsigma)\n",
    "        grads = tape.gradient(losses['total_vae_loss'], model.weights) # model.weights means variables here.\n",
    "        optimizer.apply_gradients(zip(grads, model.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if we can agree on rc-loss first.\n",
    "kl_weight = 5e-3\n",
    "tf.set_random_seed(0)\n",
    "total_loss, cache = forward_propagation_K(train_inputs, learned_params)\n",
    "sampled_vector = cache['sampled_vector']\n",
    "z_2 = cache['z_2']\n",
    "W_2 = cache['W_2']\n",
    "eps = cache['eps']\n",
    "a_1 = cache['a_1']\n",
    "z_1 = cache['z_1']\n",
    "W_mu = cache['W_mu']\n",
    "z_mu = cache['z_mu']\n",
    "W_sigma = cache['W_sigma']\n",
    "inputs = cache['inputs']\n",
    "\n",
    "grad_z_2_rc = 2* (z_2 - train_inputs) # (n_x, m)\n",
    "grad_W_2_rc = (1/m) * sampled_vector.T.dot(grad_z_2_rc) # (n_z, n_x)\n",
    "grad_b_2_rc = np.mean(grad_z_2_rc, axis=0, keepdims = True)\n",
    "grad_s_rc =  grad_z_2_rc.dot(W_2.T) # (n_z, m), gradient of sampled vector\n",
    "grad_z_mu_rc = grad_s_rc # (n_z, m)\n",
    "grad_z_sigma_rc = np.multiply(grad_s_rc, 0.5 * np.exp(z_sigma * 0.5) * eps)\n",
    "grad_W_mu_rc =  (1/m) * a_1.T.dot(grad_z_mu_rc)\n",
    "grad_b_mu_rc = np.mean(grad_z_mu_rc, axis = 0,keepdims=True)\n",
    "grad_W_sigma_rc = (1/m) * a_1.T.dot(grad_z_sigma_rc)\n",
    "grad_b_sigma_rc = np.mean(grad_z_sigma_rc, axis = 0, keepdims=True)\n",
    "\n",
    "grad_a_1_rc = grad_z_mu_rc.dot(W_mu.T) + grad_z_sigma_rc.dot(W_sigma.T)\n",
    "grad_z_1_rc = np.multiply(grad_a_1_rc, relu(z_1, derivative=True))\n",
    "grad_W_1_rc = (1/m) * inputs.T.dot(grad_z_1_rc)\n",
    "grad_b_1_rc = np.mean(grad_z_1_rc, axis = 0, keepdims = True)\n",
    "\n",
    "# This is the kl-loss gradients\n",
    "grad_b_2_kl = 0\n",
    "grad_W_2_kl = 0\n",
    "grad_z_sigma_kl =  0.5*(np.exp(z_sigma) - 1)\n",
    "grad_z_mu_kl = z_mu\n",
    "grad_W_sigma_kl = a_1.T.dot(grad_z_sigma_kl)\n",
    "grad_b_sigma_kl = np.sum(grad_z_sigma_kl, axis = 0)\n",
    "grad_W_mu_kl = a_1.T.dot(grad_z_mu_kl)\n",
    "grad_b_mu_kl = np.sum(grad_z_mu_kl, axis = 0)\n",
    "grad_a_1_kl = grad_z_mu_kl.dot(W_mu.T) + grad_z_sigma_kl.dot(W_sigma.T)\n",
    "grad_z_1_kl = np.multiply(grad_a_1_kl, relu(z_1, derivative = True))\n",
    "grad_W_1_kl = inputs.T.dot(grad_z_1_kl)\n",
    "grad_b_1_kl = np.sum(grad_z_1_kl, axis = 0, keepdims = True)\n",
    "\n",
    "# Add up the two gradients in the correct mix:\n",
    "grad_W_2 = m * grad_W_2_rc + kl_weight * grad_W_2_kl\n",
    "grad_b_2 = m * grad_b_2_rc + kl_weight * grad_b_2_kl\n",
    "grad_W_mu = m * grad_W_mu_rc + kl_weight * grad_W_mu_kl\n",
    "grad_b_mu = m * grad_b_mu_rc + kl_weight * grad_b_mu_kl\n",
    "grad_W_sigma = m * grad_W_sigma_rc + kl_weight * grad_W_sigma_kl\n",
    "grad_b_sigma = m * grad_b_sigma_rc + kl_weight * grad_b_sigma_kl\n",
    "grad_W_1 = m * grad_W_1_rc + kl_weight * grad_W_1_kl\n",
    "grad_b_1 = m * grad_b_1_rc + kl_weight * grad_b_1_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=1496, shape=(10,), dtype=float32, numpy=\n",
       " array([15.215241  ,  9.842631  , 34.036102  ,  0.04143097, 27.469017  ,\n",
       "        30.452288  ,  1.4018004 , 16.87278   ,  3.5270784 , -0.19807675],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: id=1498, shape=(20, 10), dtype=float32, numpy=\n",
       " array([[ 8.35354447e-01, -7.23051846e-01,  1.02119684e+00,\n",
       "         -4.08361852e-01,  2.74458075e+00,  3.31166816e+00,\n",
       "         -1.02228999e-01,  7.57703543e-01,  8.21092948e-02,\n",
       "         -3.08054034e-02],\n",
       "        [ 1.17832577e+00, -1.01991498e+00,  1.44046962e+00,\n",
       "         -5.76022804e-01,  3.87142229e+00,  4.67133713e+00,\n",
       "         -1.44201055e-01,  1.06879377e+00,  1.15821004e-01,\n",
       "         -4.34531830e-02],\n",
       "        [ 3.14388007e-01,  4.58721250e-01,  7.50095427e-01,\n",
       "         -4.30460423e-02,  4.13787872e-01,  3.65335196e-01,\n",
       "          3.47389653e-02,  3.78335118e-01,  6.30216226e-02,\n",
       "          8.80144886e-04],\n",
       "        [ 5.44171333e-01,  7.93996513e-01,  1.29833317e+00,\n",
       "         -7.45080560e-02,  7.16221690e-01,  6.32355332e-01,\n",
       "          6.01293556e-02,  6.54856920e-01,  1.09083556e-01,\n",
       "          1.52343279e-03],\n",
       "        [ 3.34605902e-01, -2.89622486e-01,  4.09046173e-01,\n",
       "         -1.63571611e-01,  1.09935665e+00,  1.32650721e+00,\n",
       "         -4.09483798e-02,  3.03502321e-01,  3.28893550e-02,\n",
       "         -1.23392781e-02],\n",
       "        [ 2.73454070e+00,  3.98994827e+00,  6.52431536e+00,\n",
       "         -3.74413431e-01,  3.59911871e+00,  3.17767859e+00,\n",
       "          3.02158743e-01,  3.29075170e+00,  5.48160732e-01,\n",
       "          7.65547575e-03],\n",
       "        [ 2.72056349e-02, -2.35481877e-02,  3.32581103e-02,\n",
       "         -1.32994363e-02,  8.93848836e-02,  1.07853658e-01,\n",
       "         -3.32936831e-03,  2.46767178e-02,  2.67412397e-03,\n",
       "         -1.00326352e-03],\n",
       "        [ 4.42505264e+00,  6.45656109e+00,  1.05576859e+01,\n",
       "         -6.05878830e-01,  5.82411718e+00,  5.14214087e+00,\n",
       "          4.88955319e-01,  5.32511520e+00,  8.87037337e-01,\n",
       "          1.23881446e-02],\n",
       "        [ 1.74766541e+00,  2.55000591e+00,  4.16973829e+00,\n",
       "         -2.39290535e-01,  2.30022335e+00,  2.03087807e+00,\n",
       "          1.93111897e-01,  2.10314369e+00,  3.50333691e-01,\n",
       "          4.89267707e-03],\n",
       "        [ 1.50436711e+00, -1.30212462e+00,  1.83904648e+00,\n",
       "         -7.35407889e-01,  4.94264317e+00,  5.96389389e+00,\n",
       "         -1.84101462e-01,  1.36452794e+00,  1.47868648e-01,\n",
       "         -5.54766282e-02],\n",
       "        [ 1.86387384e+00,  2.71956420e+00,  4.44699860e+00,\n",
       "         -2.55201846e-01,  2.45317268e+00,  2.16591787e+00,\n",
       "          2.05952540e-01,  2.24298882e+00,  3.73628467e-01,\n",
       "          5.21799549e-03],\n",
       "        [ 1.44279325e+00,  2.10516930e+00,  3.44234633e+00,\n",
       "         -1.97547540e-01,  1.89896023e+00,  1.67660069e+00,\n",
       "          1.59424424e-01,  1.73626041e+00,  2.89219588e-01,\n",
       "          4.03917348e-03],\n",
       "        [ 6.41880095e-01,  9.36562240e-01,  1.53145492e+00,\n",
       "         -8.78863037e-02,  8.44822824e-01,  7.45897710e-01,\n",
       "          7.09258690e-02,  7.72439599e-01,  1.28669992e-01,\n",
       "          1.79696956e-03],\n",
       "        [ 5.43102086e-01, -4.70089257e-01,  6.63927078e-01,\n",
       "         -2.65494823e-01,  1.78437757e+00,  2.15306759e+00,\n",
       "         -6.64637685e-02,  4.92617756e-01,  5.33830933e-02,\n",
       "         -2.00280081e-02],\n",
       "        [ 3.19170415e-01, -2.76262164e-01,  3.90176773e-01,\n",
       "         -1.56026036e-01,  1.04864359e+00,  1.26531494e+00,\n",
       "         -3.90594266e-02,  2.89501816e-01,  3.13721672e-02,\n",
       "         -1.17700621e-02],\n",
       "        [ 1.15534079e+00,  1.68574929e+00,  2.75651693e+00,\n",
       "         -1.58189490e-01,  1.52062404e+00,  1.34256589e+00,\n",
       "          1.27661765e-01,  1.39033926e+00,  2.31597424e-01,\n",
       "          3.23443045e-03],\n",
       "        [ 3.67604280e+00,  5.36368656e+00,  8.77063274e+00,\n",
       "         -5.03324211e-01,  4.83829451e+00,  4.27175236e+00,\n",
       "          4.06191885e-01,  4.42375660e+00,  7.36892402e-01,\n",
       "          1.02912756e-02],\n",
       "        [ 1.43596780e+00,  2.09520936e+00,  3.42606068e+00,\n",
       "         -1.96612880e-01,  1.88997591e+00,  1.66866863e+00,\n",
       "          1.58670247e-01,  1.72804606e+00,  2.87851304e-01,\n",
       "          4.02005576e-03],\n",
       "        [ 2.96235991e+00,  4.32235718e+00,  7.06786585e+00,\n",
       "         -4.05606478e-01,  3.89896727e+00,  3.44241595e+00,\n",
       "          3.27332079e-01,  3.56490970e+00,  5.93828976e-01,\n",
       "          8.29326361e-03],\n",
       "        [ 1.67067766e+00, -1.44607627e+00,  2.04235554e+00,\n",
       "         -8.16708386e-01,  5.48905897e+00,  6.62321138e+00,\n",
       "         -2.04454273e-01,  1.51537824e+00,  1.64215475e-01,\n",
       "         -6.16096631e-02]], dtype=float32)>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[5],grads[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[15.2152403 ,  9.84263355, 34.03610569,  0.04143069, 27.46901667,\n",
       "         30.45229175,  1.4018    , 16.87278543,  3.52707815, -0.1980767 ]]),\n",
       " array([[ 8.35354562e-01, -7.23051893e-01,  1.02119706e+00,\n",
       "         -4.08361892e-01,  2.74458094e+00,  3.31166809e+00,\n",
       "         -1.02229021e-01,  7.57703625e-01,  8.21093711e-02,\n",
       "         -3.08054048e-02],\n",
       "        [ 1.17832566e+00, -1.01991494e+00,  1.44046942e+00,\n",
       "         -5.76022826e-01,  3.87142213e+00,  4.67133797e+00,\n",
       "         -1.44201138e-01,  1.06879362e+00,  1.15820974e-01,\n",
       "         -4.34531642e-02],\n",
       "        [ 3.14387982e-01,  4.58721231e-01,  7.50095313e-01,\n",
       "         -4.30460557e-02,  4.13787846e-01,  3.65335146e-01,\n",
       "          3.47389503e-02,  3.78335121e-01,  6.30216206e-02,\n",
       "          8.80144578e-04],\n",
       "        [ 5.44171335e-01,  7.93996460e-01,  1.29833324e+00,\n",
       "         -7.45080314e-02,  7.16221667e-01,  6.32355324e-01,\n",
       "          6.01293372e-02,  6.54856864e-01,  1.09083557e-01,\n",
       "          1.52343435e-03],\n",
       "        [ 3.34605910e-01, -2.89622452e-01,  4.09046155e-01,\n",
       "         -1.63571624e-01,  1.09935714e+00,  1.32650705e+00,\n",
       "         -4.09484022e-02,  3.03502395e-01,  3.28893646e-02,\n",
       "         -1.23392760e-02],\n",
       "        [ 2.73454058e+00,  3.98994838e+00,  6.52431451e+00,\n",
       "         -3.74413759e-01,  3.59911867e+00,  3.17767802e+00,\n",
       "          3.02158717e-01,  3.29075155e+00,  5.48160835e-01,\n",
       "          7.65548050e-03],\n",
       "        [ 2.72056365e-02, -2.35481888e-02,  3.32581126e-02,\n",
       "         -1.32994368e-02,  8.93848848e-02,  1.07853650e-01,\n",
       "         -3.32937140e-03,  2.46767185e-02,  2.67411923e-03,\n",
       "         -1.00326338e-03],\n",
       "        [ 4.42505168e+00,  6.45656091e+00,  1.05576890e+01,\n",
       "         -6.05878826e-01,  5.82411768e+00,  5.14213963e+00,\n",
       "          4.88955238e-01,  5.32511597e+00,  8.87037496e-01,\n",
       "          1.23881493e-02],\n",
       "        [ 1.74766522e+00,  2.55000568e+00,  4.16973791e+00,\n",
       "         -2.39290617e-01,  2.30022350e+00,  2.03087765e+00,\n",
       "          1.93111883e-01,  2.10314378e+00,  3.50333666e-01,\n",
       "          4.89267451e-03],\n",
       "        [ 1.50436756e+00, -1.30212471e+00,  1.83904633e+00,\n",
       "         -7.35407949e-01,  4.94264200e+00,  5.96389400e+00,\n",
       "         -1.84101495e-01,  1.36452808e+00,  1.47868558e-01,\n",
       "         -5.54766251e-02],\n",
       "        [ 1.86387338e+00,  2.71956416e+00,  4.44699786e+00,\n",
       "         -2.55201858e-01,  2.45317313e+00,  2.16591756e+00,\n",
       "          2.05952544e-01,  2.24298892e+00,  3.73628534e-01,\n",
       "          5.21800495e-03],\n",
       "        [ 1.44279326e+00,  2.10516921e+00,  3.44234680e+00,\n",
       "         -1.97547498e-01,  1.89896036e+00,  1.67660062e+00,\n",
       "          1.59424426e-01,  1.73626027e+00,  2.89219610e-01,\n",
       "          4.03917051e-03],\n",
       "        [ 6.41879879e-01,  9.36562290e-01,  1.53145513e+00,\n",
       "         -8.78863018e-02,  8.44822664e-01,  7.45897722e-01,\n",
       "          7.09258449e-02,  7.72439519e-01,  1.28670027e-01,\n",
       "          1.79697421e-03],\n",
       "        [ 5.43102302e-01, -4.70089188e-01,  6.63927033e-01,\n",
       "         -2.65494790e-01,  1.78437791e+00,  2.15306727e+00,\n",
       "         -6.64637738e-02,  4.92617867e-01,  5.33830669e-02,\n",
       "         -2.00280061e-02],\n",
       "        [ 3.19170530e-01, -2.76262161e-01,  3.90176846e-01,\n",
       "         -1.56026061e-01,  1.04864377e+00,  1.26531524e+00,\n",
       "         -3.90594513e-02,  2.89501822e-01,  3.13721774e-02,\n",
       "         -1.17700649e-02],\n",
       "        [ 1.15534069e+00,  1.68574925e+00,  2.75651643e+00,\n",
       "         -1.58189443e-01,  1.52062408e+00,  1.34256583e+00,\n",
       "          1.27661759e-01,  1.39033928e+00,  2.31597411e-01,\n",
       "          3.23443295e-03],\n",
       "        [ 3.67604233e+00,  5.36368677e+00,  8.77063462e+00,\n",
       "         -5.03324338e-01,  4.83829447e+00,  4.27175190e+00,\n",
       "          4.06191900e-01,  4.42375662e+00,  7.36892496e-01,\n",
       "          1.02912608e-02],\n",
       "        [ 1.43596716e+00,  2.09520930e+00,  3.42606047e+00,\n",
       "         -1.96612867e-01,  1.88997605e+00,  1.66866834e+00,\n",
       "          1.58670161e-01,  1.72804573e+00,  2.87851262e-01,\n",
       "          4.02006051e-03],\n",
       "        [ 2.96235961e+00,  4.32235748e+00,  7.06786577e+00,\n",
       "         -4.05606779e-01,  3.89896711e+00,  3.44241556e+00,\n",
       "          3.27332052e-01,  3.56490942e+00,  5.93829007e-01,\n",
       "          8.29327105e-03],\n",
       "        [ 1.67067789e+00, -1.44607675e+00,  2.04235594e+00,\n",
       "         -8.16708516e-01,  5.48905926e+00,  6.62321237e+00,\n",
       "         -2.04454220e-01,  1.51537892e+00,  1.64215672e-01,\n",
       "         -6.16096579e-02]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_b_mu*1000 + grad_b_mu_kl * 5e-3 , \\\n",
    "# grad_W_mu*1000 + grad_W_mu_kl * 5e-3\n",
    "grad_b_sigma, grad_W_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10000001, -0.10000002, -0.10000001, -0.09999925, -0.1       ,\n",
       "       -0.10000001, -0.09999997, -0.10000001, -0.09999999,  0.09999985],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[5] # b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03162278,  0.03162278,  0.03162278,  0.03162277,  0.03162278,\n",
       "         0.03162278,  0.03162278,  0.03162278,  0.03162278, -0.03162278]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moment = np.divide(first_moment_update(0, grad_b_sigma, 0.9, 1),\n",
    "                  (np.sqrt(second_moment_update(0, grad_b_sigma, 0.999, 1)) + 1e-8))\n",
    "modified_alpha = alpha * (np.sqrt(1-np.power(0.999,1))/(1-np.power(0.9,1)))\n",
    "modified_alpha*moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=1496, shape=(10,), dtype=float32, numpy=\n",
       " array([15.215241  ,  9.842631  , 34.036102  ,  0.04143097, 27.469017  ,\n",
       "        30.452288  ,  1.4018004 , 16.87278   ,  3.5270784 , -0.19807675],\n",
       "       dtype=float32)>,\n",
       " array([[15.2152403 ,  9.84263355, 34.03610569,  0.04143069, 27.46901667,\n",
       "         30.45229175,  1.4018    , 16.87278543,  3.52707815, -0.1980767 ]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[5], grad_b_sigma # Both are 354.732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.06388587]]), array([[4.08140457e-05]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def first_moment_update(previous_moment, grad, beta, timestep):\n",
    "    biased = beta * previous_moment + (1 - beta) * grad\n",
    "#     unbiased = biased / (1 - np.power(beta, timestep))\n",
    "    return biased\n",
    "\n",
    "def second_moment_update(previous_moment, grad, beta, timestep):\n",
    "    biased = beta * previous_moment + (1 - beta) * np.square(grad)\n",
    "#     unbiased = biased / (1 - np.power(beta,timestep))\n",
    "    return biased\n",
    "\n",
    "first_moment_update(0, grad_b_2, 0.9,1)/10, second_moment_update(0, grad_b_2, 0.999,1)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'beta1_power:0' shape=() dtype=float32, numpy=0.7289999>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32, numpy=0.9970031>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer._get_beta_accumulators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_3/bias/Adam_14:0' shape=(1,) dtype=float32, numpy=array([-0.6388595], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias/Adam_15:0' shape=(1,) dtype=float32, numpy=array([0.0408136], dtype=float32)>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.get_slot(model.trainable_variables[7],'m'), \\\n",
    "optimizer.get_slot(model.trainable_variables[7],'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3356, shape=(1,), dtype=float32, numpy=array([-6.3885937], dtype=float32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=3605, shape=(1,), dtype=float32, numpy=array([-0.6388594], dtype=float32)>,\n",
       " array([0.04081413], dtype=float32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_moment_update(0, grads[7], 0.9,2), second_moment_update(0, grads[7], 0.999,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
