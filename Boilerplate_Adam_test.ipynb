{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of model\n",
    "Define your model here, e.g. the code below is the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(0)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_logsigma = args\n",
    "    tf.set_random_seed(0)\n",
    "    epsilon = tf.random_normal(shape = tf.shape(z_mean))\n",
    "    sampled_vector = tf.add(z_mean, tf.multiply(tf.exp(.5 * z_logsigma), epsilon))\n",
    "    return sampled_vector\n",
    "\n",
    "def total_vae_loss (x, x_pred, mu, logsigma, kl_weight =5e-3):\n",
    "    kl_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1 - logsigma, axis = 1)\n",
    "    reconstruction_loss = tf.reduce_mean((x - x_pred)**2)\n",
    "    total_vae_loss = kl_weight * kl_loss + reconstruction_loss\n",
    "    \n",
    "    losses = {'kl_loss': kl_loss,\n",
    "              'rc_loss': reconstruction_loss,\n",
    "              'total_vae_loss': total_vae_loss}\n",
    "    return losses\n",
    "\n",
    "inputs = Input(shape = n_x, batch_size = batch_size)\n",
    "a_1 = Dense(units = l_1, activation = 'relu')(inputs) \n",
    "z_mean = Dense(units = n_z)(a_1)\n",
    "z_logsigma = Dense(units = n_z)(a_1)\n",
    "sampled_vector = Lambda(sampling)([z_mean, z_logsigma])\n",
    "z_2 = Dense(units = n_x)(sampled_vector)\n",
    "model = Model(inputs = inputs, outputs = [z_2, z_mean, z_logsigma])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the model train once (1st epoch, 1st batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = alpha) # Initialize optimizer\n",
    "tf.set_random_seed(0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z_2, z_mean, z_logsigma = model(train_inputs) # Forward pass\n",
    "    losses = total_vae_loss(train_inputs, z_2, z_mean, z_logsigma) # Compute loss\n",
    "    grads = tape.gradient(losses['total_vae_loss'], model.weights) # Calculate gradient against model.weights \n",
    "                                                                   # model.weight means trainable parameters here.\n",
    "    optimizer.apply_gradients(zip(grads, model.weights)) # Apply the gradient updates to the trainable params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the gradient of the parameters of your last layer\n",
    "grad_last_bias = grads[len(grads)-1]\n",
    "\n",
    "# Obtain the moments (m and v) that is part of the ADAM schedule:\n",
    "def first_moment_update(previous_moment, grad, beta, timestep):\n",
    "    biased = beta * previous_moment + (1 - beta) * grad\n",
    "    unbiased = biased / (1 - np.power(beta, timestep))\n",
    "    return unbiased\n",
    "\n",
    "def second_moment_update(previous_moment, grad, beta, timestep):\n",
    "    biased = beta * previous_moment + (1 - beta) * np.square(grad)\n",
    "    unbiased = biased / (1 - np.power(beta,timestep))\n",
    "    return unbiased\n",
    "\n",
    "# first_moment_update returns m, second_moment_update returns v. The values for beta are default values recommended\n",
    "# in the paper, same as implemented in tf.\n",
    "first_moment_update(0, grad_last_bias, 0.9,1), second_moment_update(0, grad_last_bias, 0.999,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The m and v that tensorflow thinks is correct is here:\n",
    "optimizer.get_slot(model.trainable_variables[len(grads)-1],'m'), \\\n",
    "optimizer.get_slot(model.trainable_variables[len(grads)-1],'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the manual calculations for m and v the same as what tensorflow thinks it is?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
